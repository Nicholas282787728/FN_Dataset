What should every programmer know about functional programming?
I'm going to "squirrel the motion" a bit, as they say in the jargon of collegiate parliamentary debate, and interpret the question as "what should every programmer know about functional programming." What it is A functional program is a program that describes how to compute a function, in the mathematical sense of the word, that is, a contraption that always spits out the same output for the same input, and does nothing else except for spitting out output (and taking some time to do so). Functions in the mathematical sense are different from "functions" in most programming languages, which to avoid confusion I shall henceforth call "procedures," in that (a) not every function can be described by a procedure, since some are uncomputable, and (b) not every procedure describes a function, since some have side effects and/or return different outputs on different occasions for the same input. What it's good and bad for Why on earth would you want to artificially limit yourself to using procedures that actually represent functions as opposed to any other procedures? If you are going to give up expressive power like that, you'd better be getting some compensating advantage somewhere else, or otherwise you're just playing mathematical games that are irrelevant to practical computer programming. Fortunately, you do get such advantages. Unfortunately, there are some disadvantages too. I'll list what I think are some of the most interesting, and, in order to actually answer the question, some that are the most important to understand for somebody who is not necessarily a specialized functional programmer. PRO 1: TESTABILITY In an ideal world, maybe, programmers wouldn't have to test their code. After all, computers do what you ask them to, so all you have to do to have correct code is to ask the computer for what you want it to do. Unfortunately, real programmers are surprisingly inarticulate about what they actually want. To remedy that problem, we do the same thing we do in real life conversation if we want to be absolutely sure we are being understood correctly: we ask our interlocutor to draw some logical inferences from what we have said, and verify that they come to the same conclusion that we come to based on what we think we have said. (Teachers do this all the time. We call it "a test.") While it is possible to come to the same conclusion from different premises, you can in practice eliminate much confusion by asking a reasonably comprehensive set of such questions, and it's actually surprisingly easy to develop in intuition for what makes a sufficiently comprehensive set of questions. So suppose we have a program that sorts numbers. Once you are done writing it (or even beforehand!) you can make sure that the computer understands the same by what you said as you meant to say by giving it some particular examples of lists of numbers to sort and seeing that it sorts them the same way you think they ought to be sorted. An experienced programmer will quickly come up with some cases likely to go wrong, like the empty list, or a list that contains a given number more than once, or a list that is already sorted going in. But this sort of "unit testing" that works so nicely for classroom exercises like sorting numbers often fails horribly for real world code. The reason is that the real world code interacts with stuff, changes behavior over time, changes behavior because of being run repeatedly, etc. Suppose you live halfway between two train stations, from both of which trains run to the office you work at. You have written a procedure that tells you which train station to go to if you want to get to work as soon possible, starting now. How are you going to test this code? Any way you put it, the result is going to depend on the time of day you run your tests.  One technique in testing is to take the output and then, rather than just cehkcing it against a known good answer, do some further computation on it to determine if it's correct. But that is very dangerous, because you may well be inclined to repeat the same bug in your original code and in the code that checks if the answer is correct! A functional programmer would have a much easier time, because they would never have made the time of day something that gets implicitly looked up by interacting with the dirty, smelly realm of the gettimeofday() system call (yuck!). They would have simply passed the time in as a parameter. Of course, this is a trivial example, and it doesn't take a Haskell wizard to know why it's a good idea to pass the time in as a parameter. But the Haskell wizard would recognize that the simple testability hack of making the time a parameter is actually a much more general principle of software design: split your program into a rich, expressive functional core and a thin, boring imperative shell required to actually interact with it. And once they recognize the general principle, they may then start being able to apply it in much more complex cases where testability is traditionally very elusive. Testability is a big subject, and I can't say everything I know about it, let alone everything there is to be said about it, in one quora answer, but I hope this gives a taste for why functional programming is very relevant to testable code. PRO 2: COMPOSABILITY Suppose you are going to write a pretty-printer, that is, a program that takes some complex data structure, probably some sort of Abstract Syntax Tree, and outputs a nicely formatted human-readable representation of it. One way of doing this, which I have often encountered in real-world code, is to have a number of procedures, each for different aspects and subdivisions of the data type, that print things. One popular way seems to be to invent specialized versions of the ever popular printf family of library functions common in several programming languages, where special syntax directives get added to do things like indentation. This is all well and good as long as you have one and only one way of pretty printing things. For instance, you might decide that you want the arguments of a function all on one line: f(foo, bar, baz, quux) or that you want them on separate lines, just in case they get a little too long foo(  foo,  "a very long baaaaaaaaaaaaaaaaaaaar",  ...) But what if you want the one-line format as long as your input fits into 80-character lines, but the multi-line format when it doesn't? If you use the above-described, and common, approach, you are in trouble, because computer programs do not function backwards in time. When you hit the first of the parameters to print, say "foo", you have no idea if the whole thing is going to fit on a line, but you do have to make a decision as to whether to insert a newline character right now. You can try to optimistically march ahead on the assumption that everything will fit while somehow recording a tail of breadcrumbs to follow back for a retry if it turns out it didn't fit after all. But before you know it, Hansel will be trying to fit Gretel into the witch, or Gretel forgets to take the gingerbread out of the oven, or you just get confuzzled. Functional programmers have actually taken the pretty-printing problem and decomposed it nicely into its logical components without being so rash as to actually start printing anything while they are still having to make decisions about how to print it. By separating the layout computations from the act of printing, you gain the power to backtrack when you have to without embarrassment, or being eaten by witches. The classic paper is Phil Wadler's "A Prettier Printer" (http://homepages.inf.ed.ac.uk/wa...). CON 1: IT IS CLUMSY TO EMBED STATEFUL THINGS INTO FUNCTIONS You probably should also know a thing or two about the disadvantages of functional programming. So here is the first one. If what you are doing is naturally best modeled as a system that changes state over time, and you insist on using functional programming, you can easily end up with a functional program that emulates a stateful program in a roundabout way that is useful for teaching programming language semantics but only serves to add noise in a real program. On the other hand, you have to be very careful before you decide that you have a case of this. Say you are writing an editor; maybe a text editor, or a graphical editing program, or some other sort of editor. A dogmatic functional programmer would describe a text editor as a function that transforms an old document and an editor interaction into a new document. They would build up virtually the entire program as an implementation of this purely functional procedure, and then write as thin as possible a shell around it to provide the actual user experience, which is not a device for changing one document into another, but a single document that changes over time. So far, so bad. But now you want to add undo. If you have written your program with state hidden all over the place, undo and redo can be an absolute bear to implement. Moreover, it is very hard to know exactly when you are done: you may think you have adequately implemented undo, but who knows whether a side effect was missed somewhere? With the functional program, on the other hand, you know what you are doing. Sure, it looked clumsy at first to have to be so explicit about the state transformations that you are doing, but now that you are called upon to promote state to being a first class concept rather than simply the status quo at a point in time, you don't have to do anything, because state was first class from the get-go! CON 2: MACHINE/MODEL MISMATCH Functional programming does not correspond to the way that actual computers work. Notwithstanding the fascinating work on symbolic and functional computer architectures (see e.g. The Architecture of Symbolic Computers by Peter M. Kogge), all practical machines today have an architecture where machine instructions are inherently stateful things. The main source of friction has to do with memory management. Any moderately complex functional program, if it is going to be efficient, is going to have a lot of sharing between data structures. In the text editor example above, we would quickly run out of RAM if we made a whole new copy of the entire state of the world for every editor action. Much more likely, the state of the document would be represented as some sort of tree, and the old and the new tree would share most of the actual RAM used for storage. When data structures are nice and distinct from each other, you might feasibly be able to manage them by hand, because you can allocate and deallocate the entire data structure all at once. But when data structures share components among themselves, you have to start allocating and deallocating the components separately, which quickly proceed to take up more of your code than the actual logic of your program. So you need garbage collection. Now garbage collection has progressed enormously from the days when the way to garbage collect an early Lisp Machine was to dump the image to disk, reboot, and load the image back to RAM. But garbage collection remains a real problem in some applications. It would be seriously frustrating, for example, for the garbage collector itself to require a garbage-collected heap. Garbage collection is much complicated by multithreading alone or by real-time requirements alone, but garbage collection that is simultaneously real-time and concurrent, now there's a challenge: it can be done, and it can even be done correctly, but it is still unclear if it can really be done fast. While a typical garbage-collected program may well be about as memory-efficient as a typical manually managed program, if you have a really memory-constrained system, you are never going to beat an implementation that frees memory precisely when it is no longer needed––a moment that you can easily show is not computable in general from the text of your program alone even in cases where if you supply it manually you can write a formal proof to show you are correct. But let's set garbage collection aside for a minute. After all, general garbage collection is only really necessary for messy, heterogeneous systems with lots of different objects that can be interrelated in lots of different ways. Sometimes, we write systems that operate on very constrained data structures where the memory management can be neatly hidden away in the implementation of the data structure and need not influence in any way the code that operates on the data structure. (This is the philosophy behind the C++ standard template library, for instance.) Even in such cases, the friction between the functional model of computing and actual computers can heat things up considerably. The reason is that Moore's law is not what it used to be. Moore's law never said that computer keep getting faster. It only ever promised us that we could keep sticking more and more gates on a single chip. For a long time, the ways to translate those extra gates into better performance were pretty obvious, and moreover miniaturization did also have the effect that we could use higher clock speeds. But at this point, more and more computer architects fail to actually make new computers work faster than old ones. Instead, they merely build computer that can do more things at once. This punts the problem to the programmer who must figure out how to take the one thing they want to do and break it into mulitple smaller things that can be done at once, i.e., concurrently. Now there are data structures that are especially good at being functional and there are data structures that are especially good at being concurrent. And sadly, a lot of the time those are not the same data structures. Take the abstract data structure sometimes known as a dictionary, that is, an object where you can set the values (of arbitrary type) associated with keys (of some possibly different arbitrary type), and then look up the value you earlier associated with a given key. A splay tree is a great way of making a functional dictionary, in that you can have a big dictionary and quickly create a copy of it that is identical but for one little change, without actually having to copy the whole thing, because most of the storage is shared, and without taking very much time at all. But it is not trivial to make a splay tree to which several modifications can happen at once. Since you did not know before which little change you were going to make next, in order to arrange for the old and the new dictionary to share most of their memory you are going to have to rearrange the tree a little bit first, and the new representation of the old dictionary is actually a bit different from the old representation of the old dictionary––no better or worse, really, at representing the old dictionary, just a different way of arranging it so that it happens to have a lot in common with the new dictionary and thus be able to share most of the actual memory used. But that sort of global rearranging is precisely the opposite of how you want to design your data structure for concurrency. The general principle of designing concurrent data structures is divide and conquer: your data structure must readily split into enough pieces that can be modified independently of one another so that two changes are unlikely to effect the same piece and can therefore happen in parallel. This would most naturally lead you to something like a hash table with a lock on each bucket. But to a functional programmer, hash tables hurt. Hash tables are monolithic self-centered things that do not play nice with each other and certainly aren't very easily capable of sharing storage. Designing data structures that are good at being functional and at being concurrent is an exciting research problem, which is precisely why the practical programmer with a deadline may want to stay away from them. 